{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T07:09:43.501196Z",
     "start_time": "2019-03-09T07:08:20.024838Z"
    },
    "code_folding": [],
    "hide_input": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "from collections import Counter, namedtuple\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Initializing notebook. Please wait...', file=sys.stderr)\n",
    "\n",
    "import esper.captions as captions\n",
    "from captions.util import PostingUtil\n",
    "from esper.major_canonical_shows import MAJOR_CANONICAL_SHOWS\n",
    "from esper.widget import *\n",
    "from esper.rekall import *\n",
    "from rekall.interval_list import IntervalList\n",
    "from captions import CaptionIndex\n",
    "\n",
    "WIDGET_STYLE_ARGS = {'description_width': 'initial'}\n",
    "\n",
    "GroundTruth = namedtuple('GroundTruth', ['positive', 'negative'])\n",
    "        \n",
    "\n",
    "def extend_postings(postings, threshold):\n",
    "    # This does a merge with threshold\n",
    "    return PostingUtil.deoverlap(postings, threshold)\n",
    "\n",
    "\n",
    "def extend_postings_with_context(keys, contexts, threshold):\n",
    "    results = []\n",
    "    for key_p in keys:\n",
    "        for context_p in contexts:\n",
    "            if context_p.start >= key_p.start and context_p.start - key_p.end <= threshold:\n",
    "                key_p = PostingUtil.merge(key_p, context_p)\n",
    "        for context_p in contexts[::-1]:\n",
    "            if context_p.start <= key_p.start and key_p.start - context_p.end <= threshold:\n",
    "                key_p = PostingUtil.merge(key_p, context_p)\n",
    "        results.append(key_p)\n",
    "    return extend_postings(results, threshold)\n",
    "\n",
    "\n",
    "def filter_dict(d, keys):\n",
    "    return {k: v for k, v in d.items() if k in keys}\n",
    "\n",
    "\n",
    "TopicSegments = namedtuple('TopicSegments', [\n",
    "   'video_to_key_phrases', 'video_to_context_phrases', 'video_to_segments'\n",
    "])\n",
    "\n",
    "\n",
    "def or_queries(queries):\n",
    "    query = '|'.join('({})'.format(q) for q in queries)\n",
    "    return query\n",
    "\n",
    "\n",
    "def filter_video_qs(video_qs, filters):\n",
    "    if 'show' in filters:\n",
    "        video_qs = video_qs.filter(show__canonical_show__name=filters['show'])\n",
    "    if 'channel' in filters:\n",
    "        video_qs = video_qs.filter(channel__name=filters['channel'])\n",
    "    if 'start' in filters:\n",
    "        video_qs = video_qs.filter(time__gte=filters['start'])\n",
    "    if 'end' in filters:\n",
    "        video_qs = video_qs.filter(time__lte=filters['end'])\n",
    "    return video_qs\n",
    "\n",
    "\n",
    "def _find_segments(key_phrases, context_phrases, filters):    \n",
    "    video_qs = Video.objects.filter(duplicate=False, corrupted=False)\n",
    "    video_qs = filter_video_qs(video_qs, filters)\n",
    "    video_ids = [x['id'] for x in video_qs.values('id')]\n",
    "    \n",
    "    # Find the key locations\n",
    "    video_key_locations = {}\n",
    "    for d in captions.query_search(or_queries(key_phrases).upper(), video_ids=video_ids):\n",
    "        doc = captions.get_document(d.id)\n",
    "        doc_duration = captions.INDEX.document_duration(doc)\n",
    "        video_key_locations[d.id] = extend_postings(\n",
    "            PostingUtil.dilate(\n",
    "                d.postings, KEY_PHRASE_WINDOW_SIZE, doc_duration), 0)\n",
    "    \n",
    "    # Search for context locations\n",
    "    video_context_locations = {}\n",
    "    if len(context_phrases) > 0:\n",
    "        for d in captions.query_search(or_queries(context_phrases).upper(), \n",
    "                                       video_ids=video_key_locations.keys()):\n",
    "            video_context_locations[d.id] = list(d.postings)\n",
    "    \n",
    "    # Extend the key locations\n",
    "    video_topic_segments = {}\n",
    "    for video_id, key_postings in video_key_locations.items():\n",
    "        story_segments = extend_postings_with_context(\n",
    "            key_postings, video_context_locations.get(video_id, []),\n",
    "            CONTEXT_PHRASE_EXTEND_THRESH)\n",
    "        story_segments = list(filter(\n",
    "            lambda p: p.end - p.start >= MIN_PROPOSED_SEGMENT_LEN,\n",
    "            story_segments))\n",
    "        video_topic_segments[video_id] = story_segments\n",
    "    return TopicSegments(\n",
    "        video_key_locations, video_context_locations, video_topic_segments)\n",
    "\n",
    "\n",
    "CACHED_SEGMENTS_QUERY = None\n",
    "CACHED_SEGMENTS_RESULT = None\n",
    "\n",
    "\n",
    "def find_segments(key_phrases, context_phrases, filters):\n",
    "    print('Searching for segments...'.format(len(key_phrases), len(context_phrases)), \n",
    "          file=sys.stderr)\n",
    "    global CACHED_SEGMENTS_QUERY, CACHED_SEGMENTS_RESULT\n",
    "    if CACHED_SEGMENTS_QUERY == (key_phrases, context_phrases, filters):\n",
    "        result = CACHED_SEGMENTS_RESULT\n",
    "    else:\n",
    "        result = _find_segments(key_phrases, context_phrases, filters)\n",
    "        CACHED_SEGMENTS_QUERY = (key_phrases, context_phrases, filters)\n",
    "        CACHED_SEGMENTS_RESULT = result\n",
    "\n",
    "    coverage_seconds = sum(sum(p.end - p.start for p in l) \n",
    "                           for l in result.video_to_segments.values())\n",
    "    print('Found {} segments in {} videos covering {:0.2f} minutes.'.format(\n",
    "        sum(len(l) for l in result.video_to_segments.values()),\n",
    "        len(result.video_to_segments),\n",
    "        coverage_seconds / 60\n",
    "    ), file=sys.stderr)\n",
    "    return result\n",
    "    \n",
    "\n",
    "MIN_TOKEN_COUNT = 10000\n",
    "\n",
    "\n",
    "def propose_context_phrases(k=192, ncols=8, default_threshold=5.):\n",
    "    topic_result = find_segments(KEY_PHRASES, CONTEXT_PHRASES, get_filters())\n",
    "    \n",
    "    topic_word_counts = Counter()\n",
    "    for video_id, segments in topic_result.video_to_segments.items():\n",
    "        d = captions.get_document(video_id)\n",
    "        for p in segments:\n",
    "            topic_word_counts.update(captions.INDEX.tokens(d, p.idx, p.len))\n",
    "\n",
    "    all_words_total = sum(w.count for w in captions.LEXICON)\n",
    "    topic_words_total = sum(topic_word_counts.values())\n",
    "    \n",
    "    def filter_cond(t):\n",
    "        if t not in captions.LEXICON: \n",
    "            return False\n",
    "        w = captions.LEXICON[t]\n",
    "        return w.count > MIN_TOKEN_COUNT and w.token not in CONTEXT_PHRASES\n",
    "\n",
    "    const_expr = math.log(all_words_total) - math.log(topic_words_total) \n",
    "    log_pmis = [\n",
    "        (t, math.log(topic_word_counts[t]) - math.log(captions.LEXICON[t].count) + const_expr)\n",
    "        for t in topic_word_counts.keys() if filter_cond(t)\n",
    "    ]\n",
    "    log_pmis.sort(key=lambda x: -x[1])\n",
    "    log_pmis = log_pmis[:k]\n",
    "    \n",
    "    selections = []\n",
    "    for t, score in log_pmis:\n",
    "        token = captions.LEXICON[t].token\n",
    "        w = widgets.ToggleButton(\n",
    "            value=score >= default_threshold,\n",
    "            description=token,\n",
    "            disabled=False,\n",
    "            button_style='',\n",
    "            icon=''\n",
    "        )\n",
    "        selections.append((t, w))\n",
    "    \n",
    "    submit_button = widgets.Button(\n",
    "        description='Submit',\n",
    "        disabled=False,\n",
    "        button_style='danger'\n",
    "    )\n",
    "    def on_submit(b):\n",
    "        selected_words = []\n",
    "        for t, w in selections:\n",
    "            if w.value == True:\n",
    "                selected_words.append(captions.LEXICON[t].token)\n",
    "        clear_output()\n",
    "        print('Added {} words to the context.'.format(len(selected_words)))\n",
    "        \n",
    "        global CONTEXT_PHRASES\n",
    "        CONTEXT_PHRASES.update(selected_words)\n",
    "        sync_context_widget()\n",
    "    \n",
    "    submit_button.on_click(on_submit)\n",
    "    \n",
    "    cancel_button = widgets.Button(\n",
    "        description='Cancel',\n",
    "        disabled=False,\n",
    "        button_style=''\n",
    "    )\n",
    "    def on_cancel(b):\n",
    "        clear_output()\n",
    "    cancel_button.on_click(on_cancel)\n",
    "    \n",
    "    hboxes = []\n",
    "    for i in range(0, len(selections), ncols):\n",
    "        hboxes.append(widgets.HBox([w for _, w in selections[i:i + ncols]]))\n",
    "    vbox = widgets.VBox(hboxes)\n",
    "    display(widgets.HBox([\n",
    "        widgets.Label(\n",
    "            'Instructions: Select new context words and hit submit. '\n",
    "            '(Likely words may already be highlighted.) '),\n",
    "        submit_button, cancel_button\n",
    "    ]))\n",
    "    display(vbox)\n",
    "    \n",
    "\n",
    "def display_segments(topic_results, ground_truth, limit=1000, results_per_page=50,\n",
    "                     selection=None):\n",
    "    def is_selected(video_id):    \n",
    "        if selection == 'unlabeled':\n",
    "            return (video_id not in ground_truth.positive and \n",
    "                    video_id not in ground_truth.negative)\n",
    "        elif selection == 'positive':\n",
    "            return video_id in ground_truth.positive\n",
    "        elif selection == 'negative':\n",
    "            return video_id in ground_truth.negative\n",
    "        return True\n",
    "    \n",
    "    video_to_key_time = {\n",
    "        video_id : sum(p.end - p.start for p in postings)\n",
    "        for video_id, postings in topic_results.video_to_key_phrases.items()\n",
    "        if is_selected(video_id)\n",
    "    }\n",
    "    video_to_topic_time = {\n",
    "        video_id : sum(p.end - p.start for p in postings)\n",
    "        for video_id, postings in topic_results.video_to_segments.items()\n",
    "        if is_selected(video_id)\n",
    "    }\n",
    "    video_qs = Video.objects.filter(id__in=list(video_to_key_time.keys()), \n",
    "                                    duplicate=False, corrupted=False)\n",
    "    video_to_fps = {\n",
    "        v['id']: v['fps'] for v in video_qs.values('id', 'fps', 'channel__name')\n",
    "    }\n",
    "    if len(video_to_fps) == 0:\n",
    "        print('No videos to display', file=sys.stderr)\n",
    "        return\n",
    "    video_to_key_time = filter_dict(video_to_key_time, video_to_fps)\n",
    "    video_to_topic_time = filter_dict(video_to_topic_time, video_to_fps)\n",
    "    \n",
    "    # For display\n",
    "    video_order_all = list(sorted(\n",
    "        video_to_fps.keys(), \n",
    "        key=lambda x: -video_to_topic_time.get(x, 0)\n",
    "    ))\n",
    "    video_order = video_order_all[:limit]\n",
    "    video_ids = set(video_order)\n",
    "                       \n",
    "    def convert_time(v, t):\n",
    "        return int(t * video_to_fps[v])\n",
    "    \n",
    "    def to_intervallist(video_to_postings):\n",
    "        return {\n",
    "            video_id : IntervalList([\n",
    "                (convert_time(video_id, p.start), convert_time(video_id, p.end), None)\n",
    "                for p in postings\n",
    "            ]) \n",
    "            for video_id, postings in video_to_postings.items() \n",
    "            if video_id in video_ids\n",
    "        }\n",
    "    \n",
    "    def compute_true_time(video_id):\n",
    "        intervals = ground_truth.positive.get(video_id, [])\n",
    "        return sum(b - a for a, b in intervals)\n",
    "    \n",
    "    # Plot distribution of topic times in videos\n",
    "    def plot_dist_of_videos(video_order):\n",
    "        fig, ax1 = plt.subplots(figsize=(7,2))\n",
    "        x = np.arange(len(video_order))\n",
    "        y_pred = np.array([video_to_topic_time.get(v, 0) for v in video_order]) / 60\n",
    "       \n",
    "        ax1.plot(x, y_pred, color='purple')\n",
    "        y_true_tmp = [compute_true_time(v) for v in video_order]\n",
    "        if sum(y_true_tmp) > 0:\n",
    "            x_true = np.array([i for i, y in enumerate(y_true_tmp) if y > 0])\n",
    "            y_true = np.array([y for y in y_true_tmp if y > 0]) / 60\n",
    "            ax1.plot(x_true, y_true, 'x', color='blue')\n",
    "            y_max = max(np.max(y_pred), np.max(y_true))\n",
    "        else:\n",
    "            y_max = np.max(y_pred)\n",
    "        ax1.fill_betweenx([0, y_max], len(video_ids), alpha=0.2, color='gray')\n",
    "        ax1.set_ylabel('Minutes', color='purple')\n",
    "        ax1.tick_params('y', colors='purple')\n",
    "        ax1.set_ylim(0, y_max)\n",
    "        ax1.set_xlabel('Video Number')\n",
    "        ax1.set_xlim(0, len(video_order))\n",
    "        y_prop = np.cumsum(y_pred)\n",
    "        y_prop *= 100. / y_prop[-1]\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(x, y_prop, color='black')\n",
    "        ax2.set_ylabel('Cumulative % Minutes', color='black')\n",
    "        ax2.tick_params('y', colors='black')\n",
    "        plt.show()\n",
    "    \n",
    "    if DEBUG:\n",
    "        print('Videos (ordered by descending segment time)')\n",
    "        plot_dist_of_videos(video_order_all)\n",
    "    print('Loading {} of {} videos{}... Please wait.'.format(\n",
    "        len(video_ids), len(video_to_key_time), ' (shaded region)' if DEBUG else ''))\n",
    "    \n",
    "    # Convert to intervallists\n",
    "    video_to_key_intervals = to_intervallist(topic_results.video_to_key_phrases)\n",
    "    video_to_context_intervals = to_intervallist({\n",
    "        k: extend_postings(v, 15) \n",
    "        # Coalesce context words to reduce memory usage\n",
    "        for k, v in topic_results.video_to_context_phrases.items()\n",
    "    })\n",
    "    video_to_topic_intervals = to_intervallist(topic_results.video_to_segments)\n",
    "    video_to_commerical_intervals = qs_to_intrvllists(\n",
    "        Commercial.objects.filter(labeler__name='haotian-commercials',\n",
    "                                  video__id__in=video_ids))\n",
    "    \n",
    "    def ranges_to_intrvllist(v, ranges):\n",
    "        return IntervalList([\n",
    "            (convert_time(v, a), convert_time(v, b), None) \n",
    "            for a, b in ranges\n",
    "        ])\n",
    "    \n",
    "    video_to_labeled_pos_intervals = {\n",
    "        v: ranges_to_intrvllist(v, labels)\n",
    "        for v, labels in ground_truth.positive.items()\n",
    "        if v in video_ids\n",
    "    }\n",
    "    video_to_labeled_neg_intervals = {\n",
    "        v: ranges_to_intrvllist(v, labels)\n",
    "        for v, labels in ground_truth.negative.items()\n",
    "        if v in video_ids\n",
    "    }\n",
    "    \n",
    "    # Display results\n",
    "    result = intrvllists_to_result(\n",
    "        video_to_key_intervals, color='green', video_order=video_order)\n",
    "    add_intrvllists_to_result(result, video_to_context_intervals, color='orange')\n",
    "    add_intrvllists_to_result(result, video_to_topic_intervals, color='purple')\n",
    "    add_intrvllists_to_result(result, video_to_commerical_intervals, color='black')\n",
    "    add_intrvllists_to_result(result, video_to_labeled_pos_intervals, color='blue')\n",
    "    add_intrvllists_to_result(result, video_to_labeled_neg_intervals, color='red')\n",
    "    \n",
    "    video_widget = esper_widget(result, jupyter_keybindings=True,\n",
    "                                timeline_annotation_keys={';': 4, '\\'': 5},\n",
    "                                results_per_page=results_per_page)\n",
    "    update_button = widgets.Button(\n",
    "        description='Update ground truth',\n",
    "        disabled=False,\n",
    "        button_style='warning'\n",
    "    )\n",
    "    def on_update(b):\n",
    "        selected_idxs = set(video_widget.selected)\n",
    "        ignored_idxs = set(video_widget.ignored)\n",
    "        n_pos_segs = 0\n",
    "        n_neg_segs = 0\n",
    "        \n",
    "        def segment_is_ok(seg):\n",
    "            return 'min_frame' in seg and 'max_frame' in seg\n",
    "        \n",
    "        for i, video_id in enumerate(video_order):\n",
    "            video_fps = video_to_fps[video_id]\n",
    "            \n",
    "            pos_segments = []\n",
    "            neg_segments = []\n",
    "            if len(video_widget.groups) > 0:\n",
    "                pos_segments.extend([\n",
    "                    (\n",
    "                        int(seg['min_frame']) / video_fps, \n",
    "                        int(seg['max_frame']) / video_fps\n",
    "                    )\n",
    "                    for seg in video_widget.groups[i]['elements'][4]['segments'] \n",
    "                    if segment_is_ok(seg)\n",
    "                ])\n",
    "                neg_segments.extend([\n",
    "                    (\n",
    "                        int(seg['min_frame']) / video_fps, \n",
    "                        int(seg['max_frame']) / video_fps\n",
    "                    )\n",
    "                    for seg in video_widget.groups[i]['elements'][5]['segments'] \n",
    "                    if segment_is_ok(seg)\n",
    "                ])\n",
    "                \n",
    "            if i in selected_idxs:\n",
    "                pos_segments.extend([\n",
    "                    (p.start, p.end)\n",
    "                    for p in topic_results.video_to_segments[video_id]\n",
    "                ])\n",
    "            if i in ignored_idxs:\n",
    "                neg_segments.extend([\n",
    "                    (p.start, p.end)\n",
    "                    for p in topic_results.video_to_segments[video_id]\n",
    "                ])\n",
    "            \n",
    "            n_pos_segs += len(pos_segments)\n",
    "            if len(pos_segments) > 0:\n",
    "                if video_id not in ground_truth.positive:\n",
    "                    ground_truth.positive[video_id] = set()\n",
    "                ground_truth.positive[video_id].update(pos_segments)\n",
    "\n",
    "            n_neg_segs += len(neg_segments)\n",
    "            if len(neg_segments) > 0:\n",
    "                if video_id not in ground_truth.negative:\n",
    "                    ground_truth.negative[video_id] = set()\n",
    "                ground_truth.negative[video_id].update(neg_segments)\n",
    "\n",
    "        clear_output()\n",
    "        print('Added {} positive segments and {} negative segments.'.format(\n",
    "            n_pos_segs, n_neg_segs))\n",
    "                \n",
    "    update_button.on_click(on_update)\n",
    "    display(update_button)\n",
    "    display(video_widget)\n",
    "\n",
    "    \n",
    "def show_filter_widgets():\n",
    "    channel_filter_button = widgets.Dropdown(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        options=['All', 'CNN', 'FOXNEWS', 'MSNBC'],\n",
    "        value='All',\n",
    "        description='Channel:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    canonical_show_dropdown = widgets.Dropdown(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        options=['All'] + list(sorted(MAJOR_CANONICAL_SHOWS)),\n",
    "        value='All',\n",
    "        description='Show:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    start_date_picker = widgets.DatePicker(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        description='Start date:',\n",
    "        disabled=False\n",
    "    )\n",
    "    end_date_picker = widgets.DatePicker(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        description='End date:',\n",
    "        disabled=False\n",
    "    )\n",
    "    global FILTER_WIDGETS\n",
    "    FILTER_WIDGETS = {\n",
    "        'show': canonical_show_dropdown,\n",
    "        'channel': channel_filter_button,\n",
    "        'start_date': start_date_picker,\n",
    "        'end_date': end_date_picker\n",
    "    }\n",
    "    display(widgets.HBox([\n",
    "        channel_filter_button, canonical_show_dropdown, \n",
    "        start_date_picker, end_date_picker]))\n",
    "\n",
    "    \n",
    "def show_story_widgets():\n",
    "    status_output = widgets.Output()\n",
    "    key_widget = widgets.Textarea(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        value='',\n",
    "        layout=widgets.Layout(width='100%'),\n",
    "        placeholder='Phrases (one per line)',\n",
    "        description='Key phrases:',\n",
    "        disabled=False\n",
    "    )\n",
    "    global sync_key_widget\n",
    "    def sync_key_widget():\n",
    "        key_widget.value = '\\n'.join(sorted(KEY_PHRASES))\n",
    "        computed_height = 20 * (len(KEY_PHRASES) + 2)\n",
    "        key_widget.layout = widgets.Layout(\n",
    "            width='100%', \n",
    "            height='{}px'.format(computed_height)\n",
    "        )\n",
    "    def on_key_changed(b):\n",
    "        with status_output:\n",
    "            clear_output()\n",
    "            try:\n",
    "                global KEY_PHRASES\n",
    "                KEY_PHRASES = {\n",
    "                    t.strip() for t in key_widget.value.split('\\n')\n",
    "                    if len(t.strip()) > 0\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    key_widget.observe(on_key_changed, names='value')\n",
    "\n",
    "    context_widget = widgets.Textarea(\n",
    "        value='',\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        layout=widgets.Layout(width='100%'),\n",
    "        placeholder='Phrases (one per line)',\n",
    "        description='Context phrases:',\n",
    "        disabled=False\n",
    "    )\n",
    "    global sync_context_widget\n",
    "    def sync_context_widget():\n",
    "        context_widget.value = '\\n'.join(sorted(CONTEXT_PHRASES))\n",
    "        max_height = 250\n",
    "        computed_height = 20 * (len(CONTEXT_PHRASES) + 2)\n",
    "        context_widget.layout = widgets.Layout(\n",
    "            width='100%', \n",
    "            height='{}px'.format(min(max_height, computed_height))\n",
    "        )\n",
    "    def on_context_changed(b):\n",
    "        with status_output:\n",
    "            clear_output()\n",
    "            try:\n",
    "                global CONTEXT_PHRASES\n",
    "                CONTEXT_PHRASES = {\n",
    "                    t.strip() for t in context_widget.value.split('\\n') \n",
    "                    if len(t.strip()) > 0\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    context_widget.observe(on_context_changed, names='value')\n",
    "\n",
    "    sort_button = widgets.Button(\n",
    "        description='Sort phrases',\n",
    "        disabled=False,\n",
    "        button_style=''\n",
    "    )\n",
    "    def on_sort(b):\n",
    "        sync_key_widget()\n",
    "        sync_context_widget()\n",
    "    sort_button.on_click(on_sort)\n",
    "\n",
    "    show_filter_widgets()\n",
    "    display(key_widget)\n",
    "    display(context_widget)\n",
    "    display(sort_button)\n",
    "    display(status_output)\n",
    "    sync_key_widget()\n",
    "    sync_context_widget()\n",
    "\n",
    "                     \n",
    "def get_filters():\n",
    "    filters = {}\n",
    "    show = FILTER_WIDGETS['show'].value\n",
    "    if show != 'All':\n",
    "        filters['show'] = show\n",
    "    channel = FILTER_WIDGETS['channel'].value\n",
    "    if channel != 'All':\n",
    "        filters['channel'] = channel\n",
    "    if FILTER_WIDGETS['start_date'].value:\n",
    "        filters['start'] = FILTER_WIDGETS['start_date'].value\n",
    "    if FILTER_WIDGETS['end_date'].value:\n",
    "        filters['end'] = FILTER_WIDGETS['end_date'].value \n",
    "    return filters\n",
    "\n",
    "                     \n",
    "def show_video_controls():\n",
    "    show_videos_output = widgets.Output()\n",
    "    limit_slider = widgets.BoundedIntText(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        value=1000,\n",
    "        min=1,\n",
    "        max=10000,\n",
    "        description='Video limit:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    results_per_page_slider = widgets.BoundedIntText(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        value=40,\n",
    "        min=1,\n",
    "        max=100,\n",
    "        description='Results per page:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    show_videos_button = widgets.Button(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        description='Show videos',\n",
    "        disabled=False,\n",
    "        button_style='danger'\n",
    "    )\n",
    "    filter_videos_dropdown = widgets.Dropdown(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        options=['All', 'Unlabeled', 'Labeled Positive', 'Labeled Negative'],\n",
    "        value='All',\n",
    "        description='Videos:',\n",
    "        disabled=False\n",
    "    )\n",
    "    def on_show_videos(b):\n",
    "        with show_videos_output:\n",
    "            clear_output()\n",
    "            if filter_videos_dropdown.value == 'All':\n",
    "                selection = 'all'\n",
    "            elif filter_videos_dropdown.value == 'Unlabeled':\n",
    "                selection = 'unlabeled'\n",
    "            elif filter_videos_dropdown.value == 'Labeled Positive':\n",
    "                selection = 'positive'\n",
    "            elif filter_videos_dropdown.value == 'Labeled Negative':\n",
    "                selection = 'negative'\n",
    "            else:\n",
    "                raise Exception('Unknown option...')\n",
    "            topic_results = find_segments(KEY_PHRASES, CONTEXT_PHRASES, get_filters())\n",
    "            display_segments(\n",
    "                topic_results, GROUND_TRUTH,\n",
    "                limit=limit_slider.value,\n",
    "                results_per_page=results_per_page_slider.value,\n",
    "                selection=selection\n",
    "            )\n",
    "    show_videos_button.on_click(on_show_videos)\n",
    "    clear_videos_button = widgets.Button(\n",
    "        style=WIDGET_STYLE_ARGS,\n",
    "        description='Dismiss videos',\n",
    "        disabled=False,\n",
    "        button_style=''\n",
    "    )\n",
    "    def on_clear_videos(b):\n",
    "        with show_videos_output:\n",
    "            clear_output()\n",
    "    clear_videos_button.on_click(on_clear_videos)\n",
    "    display(widgets.HBox([\n",
    "        limit_slider, results_per_page_slider, filter_videos_dropdown]))\n",
    "    display(widgets.HBox([show_videos_button, clear_videos_button]))\n",
    "    display(show_videos_output)\n",
    "    \n",
    "    \n",
    "STORY_DIRECTORY = '/app/data/stories/'\n",
    "if not os.path.isdir(STORY_DIRECTORY):\n",
    "    os.makedirs(STORY_DIRECTORY)\n",
    "\n",
    "    \n",
    "def save_notebook_state():\n",
    "    name = input('Enter a story name: ').strip().replace(' ', '_')\n",
    "    assert name != '', 'Name cannot be empty'\n",
    "    out_path = os.path.join(STORY_DIRECTORY, '{}.json'.format(name))\n",
    "    if os.path.exists(out_path):\n",
    "        if input(\n",
    "            'File: {} already exists. Overwrite (y/N)? '.format(out_path)\n",
    "        ).strip().lower() != 'y':\n",
    "            print('Canceled by user.')\n",
    "            return\n",
    "\n",
    "    with open(out_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'key_phrases': list(KEY_PHRASES),\n",
    "            'context_phrases': list(CONTEXT_PHRASES),\n",
    "            'ground_truth': {\n",
    "                'positive_labels': {\n",
    "                    k: list(v) for k, v in GROUND_TRUTH.positive.items()\n",
    "                },\n",
    "                'negative_labels': {\n",
    "                    k: list(v) for k, v in GROUND_TRUTH.negative.items()\n",
    "                },\n",
    "            }\n",
    "        }, f)\n",
    "    print('Saved:', out_path)\n",
    "    \n",
    "    \n",
    "def load_notebook_state():\n",
    "    print('The following stories are saved:')\n",
    "    for fname in sorted(os.listdir(STORY_DIRECTORY)):\n",
    "        print('', fname.split('.')[0].replace('_', ' '))\n",
    "    \n",
    "    name = input('Enter a story to load: ').strip().replace(' ', '_')\n",
    "    in_path = os.path.join(STORY_DIRECTORY, '{}.json'.format(name))\n",
    "    with open(in_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    global KEY_PHRASES, CONTEXT_PHRASES, GROUND_TRUTH\n",
    "    KEY_PHRASES = set(data['key_phrases'])\n",
    "    CONTEXT_PHRASES = set(data['context_phrases'])\n",
    "    GROUND_TRUTH = GroundTruth(\n",
    "        {int(k): set(tuple(y) for y in v) \n",
    "         for k, v in data['ground_truth']['positive_labels'].items()},\n",
    "        {int(k): set(tuple(y) for y in v) \n",
    "         for k, v in data['ground_truth']['negative_labels'].items()}\n",
    "    )\n",
    "    print('Loaded:', in_path)\n",
    "    sync_context_widget()\n",
    "    sync_key_widget()\n",
    "\n",
    "    \n",
    "try:\n",
    "    _FACE_IDENTS\n",
    "except NameError:\n",
    "    _FACE_IDENTS = None\n",
    "def get_face_idents():\n",
    "    global _FACE_IDENTS\n",
    "    if _FACE_IDENTS is None:\n",
    "        print('Loading face identities...', file=sys.stderr)\n",
    "        with open('/app/data/stories-data/identities_by_video.json', 'r') as f:\n",
    "            _FACE_IDENTS = json.load(f)\n",
    "    else:\n",
    "        pass\n",
    "    return _FACE_IDENTS\n",
    "\n",
    "\n",
    "try:\n",
    "    _FACE_GENDERS\n",
    "except NameError:\n",
    "    _FACE_GENDERS = None\n",
    "def get_face_genders():\n",
    "    global _FACE_GENDERS\n",
    "    if _FACE_GENDERS is None:\n",
    "        print('Loading face genders...', file=sys.stderr)\n",
    "        with open('/app/data/stories-data/genders_by_video.json', 'r') as f:\n",
    "            _FACE_GENDERS = json.load(f)\n",
    "    else:\n",
    "        pass\n",
    "    return _FACE_GENDERS\n",
    "    \n",
    "    \n",
    "def init_global_variables():\n",
    "    global KEY_PHRASES, CONTEXT_PHRASES\n",
    "    try:\n",
    "        KEY_PHRASES, CONTEXT_PHRASES\n",
    "    except NameError:\n",
    "        KEY_PHRASES = set()\n",
    "        CONTEXT_PHRASES = set()\n",
    "\n",
    "    global GROUND_TRUTH\n",
    "    try:\n",
    "        GROUND_TRUTH\n",
    "    except NameError:\n",
    "        GROUND_TRUTH = GroundTruth({}, {})\n",
    "    \n",
    "init_global_variables()\n",
    "\n",
    "print('Done initializing notebook.', file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "Some constants to help with visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T07:09:43.530911Z",
     "start_time": "2019-03-09T07:09:43.504338Z"
    },
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "KEY_PHRASE_WINDOW_SIZE = 5\n",
    "CONTEXT_PHRASE_EXTEND_THRESH = 120\n",
    "MIN_PROPOSED_SEGMENT_LEN = 30\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Stories from a Lexicon\n",
    "\n",
    "Stories are retreived via lexicons of words. You can search for a story by defining a set of <b>key phrases</b>. This will find all segments in the data set where the phrases appear. \n",
    "\n",
    "For instance, if you are looking for segments about 'Hurricane Irma', then the relevant key phrases may include 'Irma' and 'Hurricane Irma'. Searching for segment will retreive all of these mentions. Note that in 'hurricane' would be a poor key phrase in this case because it will match all hurricane segments, regardless of whether they are about Irma or not. Ideally, your key phrases should be unique to the story.\n",
    "\n",
    "It can be useful to search for and visualize additional phrases, in addition to the key phrases. <b>Context phrases</b> are phrases that relevant to the story, but not unique to it. For instance, words such as 'devastation' and 'storm' will be used in the context of 'Hurricane Irma' but also in context of other hurricanes and weather disasters. A later cell will plot these words on the same timeline, and presence of these words are used to prioritize the order in which results are presented. However, adding <b>context phrases</b> will change the set of videos shown as those are defined purely by <b>key phrases</b>. \n",
    "\n",
    "<b>Instructions:</b>\n",
    "- Enter relevant filters.\n",
    "- Enter key phrases to start (required; see caption-index query syntax)\n",
    "- Enter a few context phrases (optional)\n",
    "- Show videos (see videos section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T07:09:43.635616Z",
     "start_time": "2019-03-09T07:09:43.533290Z"
    },
    "hide_input": false,
    "init_cell": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_story_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Videos\n",
    "\n",
    "Show videos and retreived topic segments with a timeline. You must have hit 'search for segments' prior to running this.\n",
    "\n",
    "Timeline colors:\n",
    "- Green = key phrases\n",
    "- Orange = context phrases\n",
    "- Purple = proposed story segment\n",
    "- Grey = commercial\n",
    "\n",
    "Timeline (Human Labeled) colors:\n",
    "- Blue = labeled positive segment\n",
    "- Red = labeled negative segment\n",
    "\n",
    "Videos will be ordered by descending amount of proposed time identified as the story.\n",
    "\n",
    "Select postive segments with <b>;</b> and negative segments with <b>'</b>. Use <b>[</b> and <b>]</b> to accept or reject all proposed story segments in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T07:09:43.715730Z",
     "start_time": "2019-03-09T07:09:43.639076Z"
    },
    "hide_input": false,
    "init_cell": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_video_controls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically propose context words\n",
    "\n",
    "Once we have some segments corresponding to the lexicon, we can use NLP to propose new context words. `propose_context_phrases()` will use statistics to suggest new context words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:59:04.612386Z",
     "start_time": "2019-03-09T03:59:02.849852Z"
    }
   },
   "outputs": [],
   "source": [
    "propose_context_phrases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Run `analysis()` to compute statistics over the story segments retreived. These graphs will respond to the filters earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T07:24:16.476124Z",
     "start_time": "2019-03-09T07:24:15.915405Z"
    },
    "hide_input": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def _analysis(topic_results):\n",
    "    video_qs = Video.objects.filter(\n",
    "        id__in=list(topic_results.video_to_key_phrases.keys()), \n",
    "        duplicate=False, corrupted=False)\n",
    "    video_to_meta = {\n",
    "        v['id']: {\n",
    "            'channel': v['channel__name'],\n",
    "            'show': v['show__canonical_show__name'],\n",
    "            'time': v['time'],\n",
    "            'fps': v['fps'],\n",
    "            'is_3y': v['threeyears_dataset'],\n",
    "            'path': v['path']\n",
    "        } for v in video_qs.values(\n",
    "            'id', 'channel__name', 'show__canonical_show__name', 'time', 'fps',\n",
    "            'threeyears_dataset', 'path'\n",
    "        )\n",
    "    }\n",
    "    if len(video_to_meta) == 0:\n",
    "        print('No videos to analyze.',\n",
    "              file=sys.stderr)\n",
    "        return\n",
    "    \n",
    "    clear_button = widgets.Button(\n",
    "        description='Clear Analysis',\n",
    "        disabled=False,\n",
    "        button_style=''\n",
    "    )\n",
    "    def on_clear(b):\n",
    "        clear_output()\n",
    "    clear_button.on_click(on_clear)\n",
    "    display(clear_button)\n",
    "    \n",
    "    channels = [c.name for c in Channel.objects.all()]\n",
    "    utc = timezone('UTC')\n",
    "    eastern = timezone('US/Eastern')\n",
    "    \n",
    "    channel_to_time = {c: 0. for c in channels}\n",
    "    channel_to_daypart_to_time = {c: np.zeros(24) for c in channels}\n",
    "    channel_to_weekday_to_time = {c: np.zeros(7) for c in channels}\n",
    "    channel_to_time_to_time = {c: defaultdict(float) for c in channels}\n",
    "    show_to_time = Counter()\n",
    "    for video_id, postings in topic_results.video_to_segments.items():\n",
    "        if video_id not in video_to_meta:\n",
    "            continue\n",
    "\n",
    "        video_topic_len = sum(p.end - p.start for p in postings)\n",
    "        channel = video_to_meta[video_id]['channel']\n",
    "        channel_to_time[channel] += video_topic_len\n",
    "        \n",
    "        video_dt = utc.localize(video_to_meta[video_id]['time']).astimezone(eastern)\n",
    "        for p in postings:\n",
    "            base_hour = video_dt.hour\n",
    "            posting_len = p.end - p.start\n",
    "            channel_to_daypart_to_time[channel][\n",
    "                (base_hour + int(p.start / 3600)) % 24\n",
    "            ] += posting_len\n",
    "            \n",
    "        channel_to_weekday_to_time[channel][video_dt.weekday()] += video_topic_len\n",
    "        channel_to_time_to_time[channel][video_dt.date()] += video_topic_len\n",
    "        \n",
    "        show = video_to_meta[video_id]['show']\n",
    "        show_to_time[(channel, show)] += video_topic_len\n",
    "        \n",
    "    print('Topic time by channel:')\n",
    "    for c in channel_to_time:\n",
    "        print('  {}: {:0.3f} hours'.format(c, channel_to_time[c] / 3600))\n",
    "        \n",
    "    print('\\nTopic time by day:')\n",
    "    def plot_timeline():\n",
    "        plt.figure(figsize=(11, 3))\n",
    "        bar_width = 1 / (len(channels) + 1)\n",
    "        for c in channels:\n",
    "            data = [x for x in sorted(channel_to_time_to_time[c].items())]\n",
    "            plt.scatter(\n",
    "                [x for x, _ in data], [y / 60 for _, y in data],\n",
    "                alpha=0.5, s=2, label=c)\n",
    "        plt.legend()\n",
    "        plt.ylabel('Minutes')\n",
    "        plt.xlabel('Day')\n",
    "        plt.show()\n",
    "    plot_timeline()\n",
    "        \n",
    "    print('\\nTopic time by daypart:')\n",
    "    def plot_daypart():\n",
    "        plt.figure(figsize=(11,3))\n",
    "        bar_width = 1 / (len(channels) + 1)\n",
    "        for i, c in enumerate(channels):\n",
    "            plt.bar(np.arange(24) + (i - 1) * bar_width,\n",
    "                    channel_to_daypart_to_time[c] / 60, \n",
    "                    width=bar_width, alpha=0.5, label=c)\n",
    "        plt.xticks(np.arange(24))\n",
    "        plt.legend()\n",
    "        plt.ylabel('Minutes')\n",
    "        plt.xlabel('Hour of Day')\n",
    "        plt.show()\n",
    "    plot_daypart()\n",
    "    \n",
    "    print('\\nTopic time by weekday:')\n",
    "    def plot_weekday():\n",
    "        plt.figure(figsize=(11,3))\n",
    "        bar_width = 1 / (len(channels) + 1)\n",
    "        for i, c in enumerate(channels):\n",
    "            plt.bar(np.arange(7) + (i - 1) * bar_width, \n",
    "                    channel_to_weekday_to_time[c] / 60, \n",
    "                    width=bar_width, alpha=0.5, label=c)\n",
    "        plt.xticks(np.arange(7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "        plt.legend()\n",
    "        plt.ylabel('Minutes')\n",
    "        plt.xlabel('Weekday')\n",
    "        plt.show()\n",
    "    plot_weekday()\n",
    "    \n",
    "    top_n = 10\n",
    "    print('\\nShows with most coverage (top-{}):'.format(top_n))\n",
    "    for (channel, show), seconds in show_to_time.most_common(top_n):\n",
    "        print('  {} ({}): {:0.1f} minutes'.format(show, channel, seconds / 60))\n",
    "\n",
    "    def join_face_labels_and_postings(labels, postings, label_len=3):\n",
    "        result = Counter()\n",
    "        try:\n",
    "            label_head = next(labels)\n",
    "            postings_head = next(postings)\n",
    "            while True:\n",
    "                if label_head[1] > postings_head.end:\n",
    "                    postings_head = next(postings)\n",
    "                elif label_head[1] + label_len < postings_head.start:\n",
    "                    label_head = next(labels)\n",
    "                else:\n",
    "                    result[label_head[0]] += label_len\n",
    "                    label_head = next(labels)\n",
    "        except StopIteration:\n",
    "            pass\n",
    "        return result\n",
    "    \n",
    "    # Time by Gender\n",
    "    face_genders = get_face_genders()\n",
    "    gender_to_time = Counter()\n",
    "    gender_to_time_host = Counter()\n",
    "    channel_to_gender_to_time = defaultdict(lambda: Counter())\n",
    "    channel_to_gender_to_time_host = defaultdict(lambda: Counter())\n",
    "    for video_id, postings in topic_results.video_to_segments.items():\n",
    "        if video_id not in video_to_meta:\n",
    "            continue\n",
    "        video_genders = face_genders.get(str(video_id), [])\n",
    "\n",
    "        # Compute for all faces\n",
    "        video_story_genders = join_face_labels_and_postings(\n",
    "            iter(video_genders), iter(postings))\n",
    "\n",
    "        gender_to_time.update(video_story_genders)\n",
    "        channel_to_gender_to_time[\n",
    "            video_to_meta[video_id]['channel']\n",
    "        ].update(video_story_genders)\n",
    "\n",
    "        # Compute for hosts only\n",
    "        video_story_genders_host = join_face_labels_and_postings(\n",
    "            filter(lambda x: x[-1] == 1, video_genders), iter(postings))\n",
    "        gender_to_time_host.update(video_story_genders_host)\n",
    "        channel_to_gender_to_time_host[\n",
    "            video_to_meta[video_id]['channel']\n",
    "        ].update(video_story_genders_host)\n",
    "    \n",
    "    def plot_gender_screen_time(data):\n",
    "        male_props = []\n",
    "        totals = []\n",
    "        for name, gender_to_time in data:\n",
    "            total = sum(gender_to_time[k] for k in gender_to_time)\n",
    "            male_prop = gender_to_time[1] / total\n",
    "            male_props.append(male_prop)\n",
    "            totals.append(total)\n",
    "        \n",
    "        x = np.arange(len(data))\n",
    "        names = [x[0] for x in data]\n",
    "        male_props = np.array(male_props)\n",
    "        totals = np.array(totals)\n",
    "        width = 0.4\n",
    "        \n",
    "        plt.figure(figsize=(11,3))\n",
    "        p1 = plt.bar(x - width / 2, male_props, width, color='lightblue',\n",
    "                     label='Men')\n",
    "        p2 = plt.bar(x + width / 2, -male_props + 1., width,\n",
    "                     color='salmon', label='Women')\n",
    "        plt.axhline(color='black')\n",
    "        plt.ylabel('Proportion')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.xticks(x, names, rotation=45, ha='right')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    print('\\nFace screen time by gender:')\n",
    "    gender_screen_time_data = []\n",
    "    gender_screen_time_data.append(('All channels', gender_to_time))\n",
    "    for channel in channel_to_gender_to_time:\n",
    "        gender_screen_time_data.append((channel, channel_to_gender_to_time[channel]))\n",
    "    gender_screen_time_data.append(('All channels (hosts)', gender_to_time_host))\n",
    "    for channel in channel_to_gender_to_time:\n",
    "        gender_screen_time_data.append(('{} (hosts)'.format(channel), \n",
    "                                        channel_to_gender_to_time_host[channel]))\n",
    "    gender_screen_time_data.append(('All channels (non-hosts)', \n",
    "                                    gender_to_time - gender_to_time_host))\n",
    "    for channel in channel_to_gender_to_time:\n",
    "        gender_screen_time_data.append(('{} (non-hosts)'.format(channel), \n",
    "                                        channel_to_gender_to_time[channel] \n",
    "                                        - channel_to_gender_to_time_host[channel]))\n",
    "        \n",
    "    plot_gender_screen_time(gender_screen_time_data)\n",
    "        \n",
    "    # Time by Identity\n",
    "    face_idents = get_face_idents()\n",
    "    ident_id_to_time = Counter()\n",
    "    for video_id, postings in topic_results.video_to_segments.items():\n",
    "        if video_id not in video_to_meta:\n",
    "            continue\n",
    "        video_idents = face_idents.get(str(video_id), [])\n",
    "        ident_id_to_time.update(join_face_labels_and_postings(iter(video_idents), iter(postings)))\n",
    "    top_n = 10\n",
    "    print('\\nPeople with most screen time (top-{}):'.format(top_n))\n",
    "    for ident_id, seconds in ident_id_to_time.most_common(top_n):\n",
    "        print('  {}: {:0.1f} minutes'.format(\n",
    "            Identity.objects.get(id=ident_id).name, \n",
    "            seconds / 60))\n",
    "\n",
    "\n",
    "def analysis():\n",
    "    topic_results = find_segments(KEY_PHRASES, CONTEXT_PHRASES, get_filters())\n",
    "    _analysis(topic_results)\n",
    "\n",
    "\n",
    "def analysis_handlabeled():\n",
    "    topic_segments = TopicSegments(\n",
    "        {\n",
    "            k: [\n",
    "              CaptionIndex.Posting(a, b, None, None) for a, b in v  \n",
    "            ] for k, v in GROUND_TRUTH.positive.items()\n",
    "        }, None, None)\n",
    "    _analysis(topic_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T07:24:18.385513Z",
     "start_time": "2019-03-09T07:24:16.614659Z"
    },
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `analysis()` only on labeled segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T07:24:33.072907Z",
     "start_time": "2019-03-09T07:24:33.044039Z"
    }
   },
   "outputs": [],
   "source": [
    "analysis_handlabeled()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving & Loading Progress\n",
    "\n",
    "Save your progress. Locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T05:48:52.488060Z",
     "start_time": "2019-02-22T05:48:40.715383Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "save_notebook_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T03:51:23.080709Z",
     "start_time": "2019-03-09T03:51:19.087572Z"
    }
   },
   "outputs": [],
   "source": [
    "load_notebook_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Demo Code\n",
    "Load a debugging lexicon..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T07:15:03.332328Z",
     "start_time": "2019-03-09T07:15:03.281443Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "KEY_PHRASES = {\n",
    "    'HURRICANE & IRMA :: 30'\n",
    "}\n",
    "CONTEXT_PHRASES = { \n",
    "    'ADVISORY', 'ATLANTIC', 'BANDS', 'BEACH', 'BOATS', 'BRACING', 'BRIDGES',\n",
    "    'CARIBBEAN', 'CATASTROPHIC', 'CATEGORY', 'CLEANUP', 'COAST', 'COASTAL',\n",
    "    'CUBA', 'DAMAGE', 'DEBRIS', 'DESTRUCTION', 'DESTRUCTIVE', 'DEVASTATED',\n",
    "    'DEVASTATING', 'DEVASTATION', 'DISASTERS', 'DOWNTOWN', 'ELECTRICITY',\n",
    "    'EVACUATE', 'EVACUATED', 'EVACUATION', 'EVACUATIONS', 'FEMA', 'FLOOD',\n",
    "    'FLOODED', 'FLOODING', 'FLORIDA', 'FORECAST', 'GUSTS', 'HARVEY', 'HURRICANE',\n",
    "    'HURRICANES', 'IMPACTED', 'IMPACTS', 'INTENSITY', 'IRMA', 'ISLAND', 'ISLANDS',\n",
    "    'JOSE', 'KEYS', 'LANDFALL', 'MANDATORY', 'METEOROLOGIST', 'MIAMI', 'MONSTER',\n",
    "    'MYERS', 'NURSING', 'ORLANDO', 'OUTAGES', 'OUTER', 'PALM', 'POWER',\n",
    "    'PREPARATION', 'PUERTO', 'RAIN', 'RAINFALL', 'RAINS', 'REBUILD',\n",
    "    'RESPONDERS', 'RESTORED', 'RICO', 'SHELTER', 'SHELTERS', 'STORM',\n",
    "    'STORMS', 'STRONGEST', 'SUPPLIES', 'SURGE', 'SUSTAINED', 'TAMPA',\n",
    "    'TIDE', 'TREES', 'TROPICAL', 'WARNINGS', 'WATER', 'WAVES', 'WIND', 'WINDS'\n",
    "}\n",
    "GROUND_TRUTH = GroundTruth({}, {})\n",
    "sync_context_widget()\n",
    "sync_key_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T22:46:23.229636Z",
     "start_time": "2019-02-25T22:46:22.896750Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "KEY_PHRASES = {\n",
    "    'MOSUL & (BATTLE | SIEGE) :: 60'\n",
    "}\n",
    "CONTEXT_PHRASES = {\n",
    "    'MOSUL', 'COMMANDERS', 'KURDISH', 'STRATEGIC', 'EXPLOSIONS', 'ENEMY',\n",
    "    'BOMBERS', 'GUNFIRE', 'CIVILIANS', 'OFFENSIVE', 'OPERATION', 'KURDS',\n",
    "    'DEFEAT', 'FIERCE', 'IRAQIS', 'PROVINCE', 'EXPLOSIVES', 'BAGHDAD',\n",
    "    'URBAN', 'BATTLES', 'DAM', 'ISIL', 'RETREAT', 'ISIS', 'COMBAT',\n",
    "    'SURROUNDED', 'TERRITORY', 'DECISIVE', 'STRIKES', 'CIVILIAN', 'OPERATIONS',\n",
    "    'BOMBINGS', 'FLEEING', 'SUNNI', 'BATTLE', 'FLEE', 'ARMY', 'COALITION',\n",
    "    'FIGHTING', 'BATTLEFIELD', 'FLED', 'CASUALTIES', 'FIGHTERS', 'IRAQI',\n",
    "    'DEFEATED', 'BOMBS', 'FORCES', 'TROOPS', 'TUNNELS', 'SIEGE', 'MILITIA',\n",
    "    'MILITANTS', 'TACTICAL', 'ARTILLERY', 'IRAQ', 'ISLAMIC', 'RESISTANCE',\n",
    "}\n",
    "GROUND_TRUTH = GroundTruth({}, {})\n",
    "sync_context_widget()\n",
    "sync_key_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T18:33:06.018896Z",
     "start_time": "2019-02-21T18:32:45.753362Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "KEY_PHRASES = {\n",
    "    '(PARKLAND | STONEMAN DOUGLAS | FLORIDA) & SHOOTING :: 60'\n",
    "}\n",
    "CONTEXT_PHRASES = {\n",
    "    'DEPUTIES', 'DEADLY', 'PARKLAND', 'HORRIFIC', 'FIREARMS', 'SHERIFF',\n",
    "    'DOUGLAS', 'GUN', 'STONEMAN', 'SURVIVOR', 'MASSACRE', 'SHOOTER',\n",
    "    'SHOOTINGS', 'FRESHMAN', 'RIFLES', 'MASS', 'CLASSES', 'SCHOOL',\n",
    "    'SHOT', 'SURVIVORS', 'SHOOTING', 'HIGH', 'FLORIDA', 'STUDENTS',\n",
    "    'TEACHERS', 'VICTIMS', 'SURVIVED', 'ORGANIZERS'\n",
    "}\n",
    "GROUND_TRUTH = GroundTruth({}, {})\n",
    "sync_context_widget()\n",
    "sync_key_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T22:33:22.123660Z",
     "start_time": "2019-02-22T22:33:21.789906Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "KEY_WORDS = {\n",
    "    'FIFA'\n",
    "}\n",
    "CONTEXT_WORDS = {\n",
    "    'ETHICS', 'INVESTIGATING', 'INDICTMENT', 'RESIGN', 'CORRUPTION',\n",
    "    'ARRESTS', 'PLEADED', 'ACCUSATION', 'SUSPENDED', 'RESIGNATION',\n",
    "    'INDICTED', 'ALLEGATION', 'SCANDAL', 'ALLEGATIONS', 'ARRESTED',\n",
    "    'SCANDALS', 'BRIBERY', 'RESIGNED', 'ABUSED', 'ACCUSATIONS', \n",
    "    'CHARGES', 'CORRUPT'\n",
    "}\n",
    "GROUND_TRUTH = GroundTruth({}, {})\n",
    "sync_context_widget()\n",
    "sync_key_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T21:56:47.359025Z",
     "start_time": "2019-02-22T21:56:47.327449Z"
    }
   },
   "outputs": [],
   "source": [
    "GROUND_TRUTH = GroundTruth({}, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
